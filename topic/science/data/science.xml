<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<documents>
	When lions attack porcupines, humans suffer unexpected consequences
	The first study of lion-porcupine interactions shows the spiny creatures can kill or seriously injure the big cats, with surprising impacts.
	In 1965, the “man-eater of Darajani” became famous after an article in Outdoor Life featured the lion’s attack on a Kenyan hunter. He wasn’t the only one—a deepening drought made the big cats desperate for prey, and there were lion attacks on other people in southern Kenya that year. But there was something curious about the Darajani lion. After he was killed, it was discovered the lion had a porcupine quill sticking out of his nose.

	In a recent investigation, led by Julian Kerbis Peterhans, a researcher at Chicago’s Roosevelt University, scientists examined this lion’s carcass and found that the quill penetrated more than six inches into the cat’s snout, nearly piercing its brain. The quill is almost certainly the reason it became a “man-eater,” Kerbis Peterhans says. With the quill in his snout, the lion had trouble hunting, became emaciated, and targeted humans out of desperation, he posits.

	This is just one conclusion from a paper by Kerbis Peterhans and colleagues, the first large-scale study of interactions between lions and porcupines. The report, published in the Journal of East African Natural History in May, suggests that lions usually avoid porcupines, unless a shortage of prey drives them to the prickly critters. These interactions can lead to death or severe injury—which in turn can prompt lions to hunt humans, cattle, and horses.

	Lions are also more likely to do so in drought years, like 1965—which was an abnormally dry one in Kenya. The team examined another lion shot that year that also had killed at least one person. It had a porcupine quill lodged in one of its fractured teeth.
	“Porcupines are not their preferred prey, for sure,” Kerbis Peterhans says. Nor are humans, he adds, though it makes sense an injured lion would try. “People are slow.”

	The conclusions are important for lion conservation, Kerbis Peterhans says. It makes it even more important for mobile veterinary units, for example, to treat lions with visible porcupine quills. It also shows the importance of porcupines as a potential cause of death, something that could get worse in areas where droughts are becoming more severe or common.

	Hunted and killed
	Nearly 70 years before the Darajani man-eater struck, a much more famous pair of lions went on a rampage in the Tsavo region, only a couple of dozen miles away. In a short period, they allegedly ate more than 100 people. These two big cats, immortalised in a famous book and the more recent film, The Ghost and the Darkness, also had porcupine quill fragments stuck in fractures within their teeth, Kerbis Peterhans says. That year, 1898, was also a drought year, and though injury by the quilled critters doesn’t appear to have driven the lions toward human-hunting in this case, it suggests that the animals were more desperate than usual.

	In all, the study documents scores of interactions between lions and porcupines. It details 40 cases where lions were seriously injured by the animals’ quills and another 10 instances where they were outright killed. This can happen when the quills pierce the heart or major arteries, Kerbis Peterhans says.

	Lions also tend to go after more porcupines during serious droughts and in arid areas where large prey are less abundant. In places with low levels of rain, for example, porcupines comprise an average of 28 percent of the big cats’ diets, whereas in wetter areas, they typically make up less than four percent, the study found.

	Young, foolish, male
	Young males generally go after porcupines more often than other lions. Solo youngsters are also more likely to be seriously injured by them, as they have no pride mates to remove the quills. (Lions will help groom each other and remove things like quills.)

	These young males are often those that have been recently kicked out of their prides by their parents and have just begun to learn how to hunt by themselves. “It’s hard to be a young animal that’s no longer being fed by its parents,” says Laurence Frank, a lion expert with the Museum of Vertebrate Zoology at the University of California, Berkeley. He adds that interactions between these two animals should be investigated more thoroughly “because it’s a common cause of serious injury, if not death, particularly to young male lions.”

	James Stevenson-Hamilton, who served as game warden of South Africa’s Sabi Game Reserve, which was expanded and renamed to Kruger National Park under his leadership, observed the same thing long ago, in the early 20th century. He noted old males rarely are wounded by porcupine quills, but “on the other hand, quite a percentage of young and those in the prime of life do, and are discovered to be thus practically incapacitated,” he wrote. “It appears to be very exceptional however, for females to commit this imprudence.”
	Porcupines’ defence
	Porcupines that live in Africa, known as crested porcupines, have spines that are not barbed but can exceed a foot in length. Besides serving obvious defence purposes, the animals may also play an “active defence,” intentionally stopping while being chased to impale their would-be predator, or jumping backward spine-first.

	Craig Packer, a researcher at the University of Minnesota, has seen porcupines wield their quills in an aggressive manner.

	“I've several times seen porcupines come up to a group of resting lions then do a 180, erect its quills, walk backwards—and get the whole group to move out of its way as it proceeded to wherever it was going,” Packer says. “The adults steered clear, though some of the subadults moved cautiously forward and tapped at it with their paws—and quickly learned that tapping at the quills was a bad idea.”
	Trees that travelled to space now live on Earth. Here's where to find them.
	Grown from hundreds of seeds that went to the moon on Apollo 14, the arboreal oddities were almost lost to history.
	Since 1977, a stately sycamore has greeted visitors to NASA’s Goddard Space Flight Centre. It looks like any other sycamore, one tree among many on the quiet, leafy campus in suburban Maryland.

	But what many passers-by may not realise as they stand under its dappled shade or admire its changing foliage is that this tree has been to the moon.

	The Goddard sycamore is one of the dozens of so-called “moon trees” scattered around the country, grown from seeds that travelled with astronaut Stuart Roosa on the Apollo 14 mission in 1971. Roosa was the command module pilot, which means that he remained in lunar orbit while commander Alan Shephard and lunar module pilot Edgar Mitchell visited the surface of the moon. During that time, Roosa had hundreds of seeds tucked inside his personal kit.

	Part scientific experiment, part public relations venture, the seeds represented a joint project between the U.S. Forest Service and NASA. Roosa had been a smoke jumper with the Forest Service before becoming an astronaut, and bringing the seeds to space helped raise awareness of the federal agency, while also giving NASA scientists an opportunity to probe an important question: How does being in microgravity affect plants?

	After splashdown, the seeds went into quarantine along with the crew. Standard protocol at the time, quarantine was meant to prevent any potentially harmful moon microbes from spreading to Earth. There was a brief moment of panic when the canister holding the seeds burst open inside a vacuum chamber, but the seeds survived the disturbance, and most of them went on to germinate normally.

	Over the next few years, the saplings were planted across the United States, many of them during 1976 as part of the U.S. bicentennial celebrations. Perhaps overshadowed by that historic anniversary, the tree plantings received relatively little national attention, and aside from some local fanfare, they largely faded into the background.

	Tipping point
	Then, about 20 years later, Dave Williams got an email. A planetary scientist at Goddard, Williams had been tasked with archiving lunar and planetary data on the agency’s website, which, like many other websites in the 1990s, was in its infancy.

	Meanwhile, a third-grade teacher from Indiana named Joan Goble had been perusing the site in search of information on a curious tree she’d found. When she saw Williams’ name, which was listed with the data he’d been cataloging online, she decided to email him. Goble explained that a class project on local trees had led her and her students to a girl scout camp near their school. There was a tree there, Goble wrote, and it had a little wooden sign on it marked “moon tree.” She asked Williams if he could elaborate on the curious moniker, and tell her if other moon trees existed.

	“I said I’ve never heard of these at all!” says Williams, who was so delighted by the query that he took on tracking down the meaning of moon trees as a personal project. “I asked around at Goddard—there’s certainly a lot of old timers there who worked on the Apollo program,” he says. “I asked everyone, and no one had heard of [moon trees].”

	Williams poked around online and asked NASA’s history office for help—a search that yielded the locations of roughly half a dozen other moon trees, and some old news clippings that mentioned the Apollo 14 moon trees. He also came across a 1976 telegram from President Gerald Ford that was sent to various bicentennial moon tree planting ceremonies, part of which read: “I send warm greetings to those who attend this unique ceremony dedicating a small tree which was taken from earth to the moon on January 31, 1971 aboard Apollo 14. This tree which was carried by Astronauts Stuart Roosa, Alan Shepard and Edgar Mitchell on their mission to the moon, is a living symbol of our spectacular human and scientific achievements.”
	Williams shared what he learned with Goble, and then had an idea.

	“I was thinking this is such a cool story,” he says, so he built a page on the NASA archive site where readers could learn about the Apollo moon trees. He also invited people to help him in his search. “At the bottom I said, If you happen to know of a moon tree near you—because there were no records kept of where they were planted—send me an email and let me know,” he says. “And I started getting emails!”

	Williams’ moon tree page is still going strong, with a current tally of 80 verified trees. Some entries, like a loblolly pine planted at the White House, are marked with an asterisk to note that the tree has died. Others, like a Douglas fir that successfully sprouted at the State Capitol Building in Salem, Oregon, include a link to extensive information about the tree and its location.

	Sowing seeds above
	Beyond showing that many of the seeds seemed to grow just fine back on Earth, the moon trees were never formally studied, so they don’t provide much insight on the botanical effects of a jaunt around the moon. Fortunately, NASA didn’t stop at saplings.

	For instance, at Kennedy Space Center in Florida, researchers are actively experimenting with crops being grown on the International Space Station.

	“The biggest challenge we have right now is water, and how to water your plants,” says Gioia Massa, a life sciences project scientist there. The current workaround requires astronauts to hand-water plants by squirting the liquid through a syringe into the clay substrate where the seeds are growing. “It’s all manual, and it’s a lot of work,” she says.

	But the rewards are sweet, or, in this case, spicy. Since 2015, astronauts have been free to eat what they grow, Massa says, and right now on the ISS they’re enjoying mizuna mustard, a leafy green that has a peppery bite.

	“They get to eat half of it,” Massa says, “and the rest comes back for science.”
	One of the things Massa and her team study is food safety. In the closed environment of a spacecraft, the ecosystem consists of humans, plants, and microorganisms, and managing that balance is key. Massa also studies plants’ chemical and nutritional profiles, and whether future crop-growing on Mars could include making use of the red planet’s own dirt and minerals. 

	Another potential key to farming beyond Earth involves capitalising on a process called evapotranspiration.

	“That means that the water they’re taking up is evaporated out of their leaves as kind of ultrapure water vapour,” Massa says. “That’s something we can collect and reuse, so we can think of plants as this really integral component of the future life-support system. Especially in the long term, when we’re trying to begin independence from Earth.”

	Of course, it may be quite a while yet before humans—and trees—are thriving on Mars. But in the meantime, aspiring astronauts can visit a moon tree and perhaps feel a little bit closer to that grand off-world adventure.
	Ancient DNA may reveal origin of the Philistines
	Historical accounts and archaeology agree that the biggest villains of the Hebrew Bible were ‘different’—but how different were they really?
	THE FIRST-EVER STUDY of DNA recovered from an ancient Philistine site is providing a unique genetic insight into the origins of some of the most notorious troublemakers of the Old Testament.

	The authors of the Hebrew Bible made it clear that the Philistines were not like them: This "uncircumcised" group is described in several passages as coming from the "Land of Caphtor" (modern-day Crete) before taking control of the coastal region of what is now southern Israel and the Gaza Strip. They warred with their Israelite neighbors, even seizing the Ark of the Covenant for a time. Their representatives in the Bible include the giant Goliath, who was felled by the future king David, and Delilah, who robbed the Israelite Samson of his strength by cutting his hair.

	Modern archaeologists agree that the Philistines were different from their neighbors: Their arrival on the eastern shores of the Mediterranean in the early 12th century B.C. is marked by pottery with close parallels to the ancient Greek world, the use of an Aegean—instead of a Semitic—script, and the consumption of pork.
	Many researchers also tie the presence of the Philistines to the exploits of the Sea Peoples, a mysterious confederation of tribes that, according to Egyptian and other historical sources, appears to have wreaked havoc across the eastern Mediterranean at the end of the Late Bronze Age in the 13th century and early 12th century B.C.

	Now, a study published today in the journal Science Advances, prompted by the unprecedented 2016 discovery of a cemetery at the ancient Philistine city of Ashkelon on the southern coast of Israel, provides an intriguing look into the genetic origins and legacy of the Philistines. The research appears to support their foreign origin, but reveals that the reviled outsiders were soon marrying into the local populations.

	The study analyzed DNA from ten sets of human remains recovered from Ashkelon across three different time periods: a Middle/Late Bronze Age burial ground (about 1650-1200 B.C.), which pre-dates the Philistine presence in the area; infant burials from the late 1100s B.C., following the arrival of the Philistines in the early Iron Age; and individuals buried in the Philistine cemetery in the later Iron Age (10th and ninth centuries B.C.)
	The four early Iron Age DNA samples, all from infants buried beneath the floors of Philistine houses, include proportionally more “additional European ancestry” in their genetic signatures (roughly 14%) than in the pre-Philistine Bronze Age samples (2% to 9%), according to the researchers. While the origins of this additional “European ancestry” are not conclusive, the most plausible models point to Greece, Crete, Sardinia, and the Iberian peninsula.
	Daniel Master, director of the Leon Levy Expedition to Ashkelon and a co-author of the study, hails the results as “direct evidence” that supports the theory that the Philistines began as migrants from the west who settled in Ashkelon in the 12th century B.C.

	“It fits with the Egyptian and other texts that we have, and it fits with the [archaeological material]."

	What researchers find even more unusual is that this specific “European blip” disappears quickly and is statistically insignificant in the DNA from study samples recovered from the cemetery at Ashkelon only a few centuries after the infant burials. The later Philistine burials have genetic signatures very similar to local populations who lived in the region before the Philistines showed up.

	“We managed to catch this movement of people coming to Ashkelon from southern Europe,” says Michal Feldman, an archaeogeneticist at the Max Planck Institute and study co-author. “Then it disappears very quickly within 200 years, probably because [the Philistines] intermarried and this kind of genetic signature was diluted within the local population.”

	“For more than a century, we have debated the question of where the Philistines came from,” writes archaeologist Eric Cline in an email from his excavation at the Canaanite site of Tel Kabri (Cline was not involved in the current research.) “Now we have the answer: Southern Europe, and probably more specifically mainland Greece, Crete, or Sardinia. This fits with what had seemed the most likely answer previously, especially judging from [the archaeological remains], and so this seems a logical finding."

	Aren Maeir, an archaeologist who directs excavations at the Philistine city of Tell-es-Safi/Gath and who was also not involved in the current research, warns against oversimplifying the story of the Philistines, however, calling the Biblical villians “an ‘entagled’ or ‘transcultural’ group, consisting of peoples of various origins.”

	“[W]hile I fully agree that there was a significant [foreign] component among the Philistines in the early Iron Age, these foreign components were not of one origin, and, no less important, they mixed with local Levantine populations from the early Iron Age onwards,” Maeir writes in an email.

	For Master, what’s most interesting is the fact that—despite the quick genetic assimilation that the Philistines underwent —they remained a distinct cultural group that was clearly identifiable from their neighbors for more than five centuries, until they were conquered by the Babylonians in 604 B.C.

	“It's kind of interesting how you see [the Philistine genetic mix] has changed so quickly,” the archaeologist observed. “Because if you were only relying on the Hebrew texts, you’d think that nobody would want to mix with the Philistines, right?”
	The first Europeans weren’t who you might think
	Genetic tests of ancient settlers' remains show that Europe is a melting pot of bloodlines from Africa, the Middle East, and today's Russia.
	Three waves of immigrants settled prehistoric Europe. The last, some 5,000 years ago, were the Yamnaya, horse-riding cattle herders from Russia who built imposing grave mounds like this one near Žabalj, Serbia.
	The idea that there were once “pure” populations of ancestral Europeans, there since the days of woolly mammoths, has inspired ideologues since well before the Nazis. It has long nourished white racism, and in recent years it has stoked fears about the impact of immigrants: fears that have threatened to rip apart the European Union and roiled politics in the United States.

	Now scientists are delivering new answers to the question of who Europeans really are and where they came from. Their findings suggest that the continent has been a melting pot since the Ice Age. Europeans living today, in whatever country, are a varying mix of ancient bloodlines hailing from Africa, the Middle East, and the Russian steppe.

	The evidence comes from archaeological artifacts, from the analysis of ancient teeth and bones, and from linguistics. But above all it comes from the new field of paleogenetics. During the past decade it has become possible to sequence the entire genome of humans who lived tens of millennia ago. Technical advances in just the past few years have made it cheap and efficient to do so; a well-preserved bit of skeleton can now be sequenced for around $500.

	The result has been an explosion of new information that is transforming archaeology. In 2018 alone, the genomes of more than a thousand prehistoric humans were determined, mostly from bones dug up years ago and preserved in museums and archaeological labs. In the process any notion of European genetic purity has been swept away on a tide of powdered bone.

	Analysis of ancient genomes provides the equivalent of the personal DNA testing kits available today, but for people who died long before humans invented writing, the wheel, or pottery. The genetic information is startlingly complete: Everything from hair and eye color to the inability to digest milk can be determined from a thousandth of an ounce of bone or tooth. And like personal DNA tests, the results reveal clues to the identities and origins of ancient humans’ ancestors—and thus to ancient migrations.
	Three major movements of people, it now seems clear, shaped the course of European prehistory. Immigrants brought art and music, farming and cities, domesticated horses and the wheel. They introduced the Indo-European languages spoken across much of the continent today. They may have even brought the plague. The last major contributors to western and central Europe’s genetic makeup—the last of the first Europeans, so to speak—arrived from the Russian steppe as Stonehenge was being built, nearly 5,000 years ago. They finished the job.

	In an era of debate over migration and borders, the science shows that Europe is a continent of immigrants and always has been. “The people who live in a place today are not the descendants of people who lived there long ago,” says Harvard University paleogeneticist David Reich. “There are no indigenous people—anyone who hearkens back to racial purity is confronted with the meaninglessness of the concept.”
	First wave: Out of Africa
	Thirty-two years ago the study of the DNA of living humans helped establish that we all share a family tree and a primordial migration story: All people outside Africa are descended from ancestors who left that continent more than 60,000 years ago. About 45,000 years ago, those first modern humans ventured into Europe, having made their way up through the Middle East. Their own DNA suggests they had dark skin and perhaps light eyes.

	Europe then was a forbidding place. Mile-thick ice sheets covered parts of the continent. Where there was enough warmth, there was wildlife. There were also other humans, but not like us: Neanderthals, whose own ancestors had wandered out of Africa hundreds of thousands of years earlier, had already adapted to the cold and harsh conditions.

	The first modern Europeans lived as hunters and gatherers in small, nomadic bands. They followed the rivers, edging along the Danube from its mouth on the Black Sea deep into western and central Europe. For millennia, they made little impact. Their DNA indicates they mixed with the Neanderthals—who, within 5,000 years, were gone. Today about 2 percent of a typical European’s genome consists of Neanderthal DNA. A typical African has none.

	As Europe was gripped by the Ice Age, the modern humans hung on in the ice-free south, adapting to the cold climate. Around 27,000 years ago, there may have been as few as a thousand of them, according to some population estimates. They subsisted on large mammals such as mammoths, horses, reindeer, and aurochs—the ancestors of modern cattle. In the caves where they sheltered, they left behind spectacular paintings and engravings
	of their prey.
	About 14,500 years ago, as Europe began to warm, humans followed the retreating glaciers north. In the ensuing millennia, they developed more sophisticated stone tools and settled in small villages. Archaeologists call this period the Mesolithic, or Middle Stone Age.

	In the 1960s Serbian archaeologists uncovered a Mesolithic fishing village nestled in steep cliffs on a bend of the Danube, near one of the river’s narrowest points. Called Lepenski Vir, the site was an elaborate settlement that had housed as many as a hundred people, starting roughly 9,000 years ago. Some dwellings were furnished with carved sculptures that were half human, half fish.

	Bones found at Lepenski Vir indicated that the people there depended heavily on fish from the river. Today what remains of the village is preserved under a canopy overlooking the Danube; sculptures of goggle-eyed river gods still watch over ancient hearths. “Seventy percent of their diet was fish,” says Vladimir Nojkovic, the site’s director. “They lived here almost 2,000 years, until farmers pushed them out.”
	Second wave: Out of Anatolia
	The Konya Plain in central Anatolia is modern Turkey’s breadbasket, a fertile expanse where you can see rainstorms blotting out mountains on the horizon long before they begin spattering the dust around you. It has been home to farmers, says University of Liverpool archaeologist Douglas Baird, since the first days of farming. For more than a decade Baird has been excavating a prehistoric village here called Boncuklu. It’s a place where people began planting small plots of emmer and einkorn, two ancient forms of wheat, and probably herding small flocks of sheep and goats, some 10,300 years ago, near the dawn of the Neolithic period.

	Within a thousand years the Neolithic revolution, as it’s called, spread north through Anatolia and into southeastern Europe. By about 6,000 years ago, there were farmers and herders all across Europe.

	It has long been clear that Europe acquired the practice of farming from Turkey or the Levant, but did it acquire farmers from the same places? The answer isn’t obvious. For decades, many archaeologists thought a whole suite of innovations—farming, but also ceramic pottery, polished stone axes capable of clearing forests, and complicated settlements—was carried into Europe not by migrants but by trade and word of mouth, from one valley to the next, as hunter-​gatherers who already lived there adopted the new tools and way of life.

	But DNA evidence from Boncuklu has helped show that migration had a lot more to do with it. The farmers of Boncuklu kept their dead close, burying them in the fetal position under the floors of their houses. Beginning in 2014, Baird sent samples of DNA extracted from skull fragments and teeth from more than a dozen burials to DNA labs in Sweden, Turkey, the U.K., and Germany.

	Many of the samples were too badly degraded after spending millennia in the heat of the Konya Plain to yield much DNA. But then Johannes Krause and his team at Germany’s Max Planck Institute for the Science of Human History tested the samples from a handful of petrous bones. The petrous bone is a tiny part of the inner ear, not much bigger than a pinkie tip; it’s also about the densest bone in the body. Researchers have found that it preserves genetic information long after usable DNA has been baked out of the rest of a skeleton. That realization, along with better sequencing machines, has helped drive the explosion in ancient DNA studies.
	The Boncuklu petrous bones paid off: DNA extracted from them was a match for farmers who lived and died centuries later and hundreds of miles to the northwest. That meant early Anatolian farmers had migrated, spreading their genes as well as their lifestyle.

	They didn’t stop in southeastern Europe. Over the centuries their descendants pushed along the Danube past Lepenski Vir and deep into the heart of the continent. Others traveled along the Mediterranean by boat, colonizing islands such as Sardinia and Sicily and settling southern Europe as far as Portugal. From Boncuklu to Britain, the Anatolian genetic signature is found wherever farming first appears.

	Those Neolithic farmers mostly had light skin and dark eyes—the opposite of many of the hunter-gatherers with whom they now lived side by side. “They looked different, spoke different languages … had different diets,” says Hartwick College archaeologist David Anthony. “For the most part, they stayed separate.”
	Across Europe, this creeping first contact was standoffish, sometimes for centuries. There’s little evidence of one group taking up the tools or traditions of the other. Even where the two populations did mingle, intermarriage was rare. “There’s no question they were in contact with each other, but they weren’t exchanging wives or husbands,” Anthony says. “Defying every anthropology course, people were not having sex with each other.” Fear of the other has a long history.

	About 5,400 years ago, everything changed. All across Europe, thriving Neolithic settlements shrank or disappeared altogether. The dramatic decline has puzzled archaeologists for decades. “There’s less stuff, less material, less people, less sites,” Krause says. “Without some major event, it’s hard to explain.” But there’s no sign of mass conflict or war.

	After a 500-year gap, the population seemed to grow again, but something was very different. In southeastern Europe, the villages and egalitarian cemeteries of the Neolithic were replaced by imposing grave mounds covering lone adult men. Farther north, from Russia to the Rhine, a new culture sprang up, called Corded Ware after its pottery, which was decorated by pressing string into wet clay.

	The State Museum of Prehistory in Halle, Germany, has dozens of Corded Ware graves, including many that were hastily rescued by archaeologists before construction crews went to work. To save time and preserve delicate remains, the graves were removed from the ground in wooden crates, soil and all, and stored in a warehouse for later analysis. Stacked to the ceiling on steel shelves, they’re now a rich resource for geneticists.
	Corded Ware burials are so recognizable, archaeologists rarely need to bother with radiocarbon dating. Almost invariably, men were buried lying on their right side and women lying on their left, both with their legs curled up and their faces pointed south. In some of the Halle warehouse’s graves, women clutch purses and bags hung with canine teeth from dozens of dogs; men have stone battle-axes. In one grave, neatly contained in a wooden crate on the concrete floor of the warehouse, a woman and child are buried together.

	When researchers first analyzed the DNA from some of these graves, they expected the Corded Ware folk would be closely related to Neolithic farmers. Instead, their DNA contained distinctive genes that were new to Europe at the time—but are detectable now in just about every modern European population. Many Corded Ware people turned out to be more closely related to Native Americans than to Neolithic European farmers. That deepened the mystery of who they were.
	Third wave: Out of the Steppe
	One bright October Morning near the Serbian town of Žabalj, Polish archaeologist Piotr Włodarczak and his colleagues steer their pickup toward a mound erected 4,700 years ago. On the plains flanking the Danube, mounds like this one, a hundred feet across and 10 feet high, provide the only topography. It would have taken weeks or months for prehistoric humans to build each one. It took Włodarczak’s team weeks of digging with a backhoe and shovels to remove the top of the mound.

	Standing on it now, he peels back a tarp to reveal what’s underneath: a rectangular chamber containing the skeleton of a chieftain, lying on his back with his knees bent. Impressions from the reed mats and wood beams that formed the roof of his tomb are still clear in the dark, hard-packed earth.

	“It’s a change of burial customs around 2800 B.C.,” Włodarczak says, crouching over the skeleton. “People erected mounds on a massive scale, accenting the individuality of people, accenting the role of men, accenting weapons. That’s something new in Europe.”

	It was not new 800 miles to the east, however. On what are now the steppes of southern Russia and eastern Ukraine, a group of nomads called the Yamnaya, some of the first people in the world to ride horses, had mastered the wheel and were building wagons and following herds of cattle across the grasslands. They built few permanent settlements. But they buried their most prominent men with bronze and silver ornaments in mighty grave mounds that still dot the steppes.

	By 2800 B.C, archaeological excavations show, the Yamnaya had begun moving west, probably looking for greener pastures. Włodarczak’s mound near Žabalj is the westernmost Yamnaya grave found so far. But genetic evidence, Reich and others say, shows that many Corded Ware people were, to a large extent, their descendants. Like those Corded Ware skeletons, the Yamnaya shared distant kinship with Native Americans—whose ancestors hailed from farther east, in Siberia.

	Within a few centuries, other people with a significant amount of Yamnaya DNA had spread as far as the British Isles. In Britain and some other places, hardly any of the farmers who already lived in Europe survived the onslaught from the east. In what is now Germany, “there’s a 70 percent to possibly 100 percent replacement of the local population,” Reich says. “Something very dramatic happens 4,500 years ago.”

	Until then, farmers had been thriving in Europe for millennia. They had settled from Bulgaria all the way to Ireland, often in complex villages that housed hundreds or even thousands of people. Volker Heyd, an archaeologist at the University of Helsinki, Finland, estimates there were as many as seven million people in Europe in 3000 B.C. In Britain, Neolithic people were constructing Stonehenge.

	To many archaeologists, the idea that a bunch of nomads could replace such an established civilization within a few centuries has seemed implausible. “How the hell would these pastoral, decentralized groups overthrow grounded Neolithic society, even if they had horses and were good warriors?” asks Kristian Kristiansen, an archaeologist at the University of Gothenburg in Sweden.

	A clue comes from the teeth of 101 people living on the steppes and farther west in Europe around the time that the Yamnaya’s westward migration began. In seven of the samples, alongside the human DNA, geneticists found the DNA of an early form of Yersinia pestis—the plague microbe that killed roughly half of all Europeans in the 14th century.

	Unlike that flea-borne Black Death, this early variant had to be passed from person to person. The steppe nomads apparently had lived with the disease for centuries, perhaps building up immunity or resistance—much as the Europeans who colonized the Americas carried smallpox without succumbing to it wholesale. And just as smallpox and other diseases ravaged Native American populations, the plague, once introduced by the first Yamnaya, might have spread rapidly through crowded Neolithic villages. That could explain both their surprising collapse and the rapid spread of Yamnaya DNA from Russia to Britain.

	“Plague epidemics cleared the way for the Yamnaya expansion,” says Morten Allentoft, an evolutionary biologist at the Natural History Museum of Denmark, who helped identify the ancient plague DNA.

	But that theory has a major question: Evidence of plague has only just recently been documented in ancient Neolithic skeletons, and so far, no one has found anything like the plague pits full of diseased skeletons left behind after the Black Death. If a plague wiped out Europe’s Neolithic farmers, it left little trace.

	Whether or not they brought plague, the Yamnaya did bring domesticated horses and a mobile lifestyle based on wagons into Stone Age Europe. And in bringing innovative metal weapons and tools, they may have helped nudge Europe toward the Bronze Age.
	That might not have been the Yamnaya’s most significant contribution to Europe’s development. Their arrival on the continent matches the time linguists pinpoint as the initial spread of Indo-European languages, a family of hundreds that includes most languages spoken from Ireland to Russia to the northern half of India. All are thought to have evolved from a single proto-Indo-European tongue, and the question of where it was spoken and by whom has been debated since the 19th century. According to one theory, it was the Neolithic farmers from Anatolia who brought it into Europe along with farming.

	Another theory, proposed a century ago by a German scholar named Gustaf Kossinna, held that the proto-Indo-Europeans were an ancient race of north Germans—the people who made Corded Ware pots and axes. Kossinna thought that the ethnicity of people in the past—their biological identity, in effect—could be deduced from the stuff they left behind.

	“Sharply defined archaeological cultural areas,” he wrote, “correspond unquestionably with the areas of particular people or tribes.”

	The north German tribe of proto-Indo-Europeans, Kossinna argued, had moved outward and dominated an area that stretched most of the way to Moscow. Nazi propagandists later used that as an intellectual justification for the modern Aryan “master race” to invade eastern Europe.

	Partly as a result, for decades after World War II the whole idea that ancient cultural shifts might be explained by migrations fell into ill repute in some archaeological circles. Even today it makes some archaeologists uncomfortable when geneticists draw bold arrows across maps of Europe.

	“This kind of simplicity leads back to Kossinna,” says Heyd, who’s German. “It calls back old demons of blond, blue-eyed guys coming back somehow out of the hell where they were sent after World War II.”

	Yet ancient DNA, which provides direct information about the biology of ancient humans, has become a strong argument against Kossinna’s theory. First, in documenting the spread of the Yamnaya and their descendants deeper and deeper into Europe at just the right time, the DNA evidence supports the favored theory among linguists: that proto-Indo-Europeans migrated into Europe from the Russian steppe, not the other way around. Second, together with archaeology it amounts to a rejection of Kossinna’s claim that some kind of pure race exists in Europe, one that can be identified from its cultural artifacts.

	All Europeans today are a mix. The genetic recipe for a typical European would be roughly equal parts Yamnaya and Anatolian farmer, with a much smaller dollop of African hunter-gatherer. But the average conceals large regional variations: more “eastern cowboy” genes in Scandinavia, more farmer ones in Spain and Italy, and significant chunks of hunter-gatherer DNA in the Baltics and eastern Europe.

	“To me, the new results from DNA are undermining the nationalist paradigm that we have always lived here and not mixed with other people,” Gothenburg’s Kristiansen says. “There’s no such thing as a Dane or a Swede or a German.” Instead, “we’re all Russians, all Africans.”
	THE ATLAS OF MOONS
	Our solar system collectively hosts nearly 200 known moons, some of which are vibrant worlds in their own right. Take a tour of the major moons in our celestial menagerie, including those that are among the most mystifying—or scientifically intriguing—places in our local neighborhood.
	Earth System
	Among the eight major planets, Earth is the only one with a single moon. Over millennia, our planet’s celestial companion has assumed many roles: Its shifting, glowing face has inspired myths and legends; its cratered surface continues to be a prime destination for robotic and human explorers; and its gravitational heft and silvery light tangibly affect life on this planet.
	Moon
	Likely born from a fiery collision between Earth and a Mars-size planet, the moon was once a quivering mass of magma. Over eons, it solidified into the orb we see today. Whether crescent, half, or full, the moon keeps one face perpetually pointed toward Earth. Its near-side complexion is a mottled mix of dark patches—the sites of ancient lava seas—and gleaming, cratered landscapes. The lunar far side is completely different, with a mix of highlands and complex terrains.

	The moon has been known throughout recorded history. The moon's largest features are named using Latin terms describing large bodies of water.
	Mars System
	Two dark, lumpy moons orbit Mars, and scientists suspect the planet stole them from the nearby asteroid belt. Phobos and Deimos, both discovered by the persistent moon hunter Asaph Hall, are among the smallest moons in the solar system. But in the future, they may become crucial way stations for humans attempting to establish a permanent presence on the Martian surface.
	Phobos
	Resembling a ruddy space potato, Phobos is snuggled close to Mars, looping around the planet three times each Martian day. But that orbital embrace won’t last forever: Phobos is slowly falling from the sky, and in another 50 million years or so it either will be ripped into a ring by Mars’s gravity or will crash into the planet’s surface.

	Discovered in 1877 by Asaph Hall, it was named after a son of Ares. Features are named for deceased scientists who studied Martian satellites, as well as people and places from Jonathan Swift's Gulliver's Travels.
	Deimos
	Smaller than Phobos and farther from Mars, irregularly shaped Deimos swings around the planet once every 30 hours. It has two named surface features, Swift and Voltaire, both honoring writers who hypothesized the existence of Martian moons before they were discovered. Unlike Phobos, Deimos is slowly drifting away from Mars and will eventually escape the planet’s gravity.

	Discovered in 1877 by Asaph Hall, it was named after a son of Ares. Features are named for deceased authors who wrote about Martian satellites.
	Jupiter System
	Jupiter’s 70 or so moons are dominated by the four Galilean satellites—Io, Europa, Ganymede, and Callisto—that were discovered by Galileo Galilei in 1610. By watching their movements over several nights, the famed Italian astronomer determined that the four prominent points of light flanking Jupiter were not distant stars, but worlds that swore gravitational allegiance to the solar system’s biggest planet.

	4MAJOR MOONS63MINOR MOONS
	Io
	The most geologically active body in the solar system, Io’s sulfuric plains are pockmarked by an estimated 400 active volcanoes, with mountains taller than Mount Everest. Some of these volcanoes spew exotic lavas and send fountains of gases 300 miles into space; particles launched by Io’s volcanoes also help power Jupiter’s blazing polar auroras.

	Discovered in 1610 by Galileo Galilei, it was named after a lover of Zeus. Volcanic features are named for ancient gods of fire, thunder, and sun.
	Europa
	With a vast global ocean tucked beneath a smooth, icy shell, Europa is considered one of the best places to look for life beyond Earth. Its ancient, alien sea likely contains all the ingredients needed for life as we know it. Peering beneath that crisscrossed crust is a bit tricky, but scientists recently spotted plumes of possible seawater venting into space, which could be sampled by an orbiting craft.

	Discovered in 1610 by Galileo Galilei, it was named after a lover of Zeus. Features are named for people, places, gods, or objects from Celtic myths, as well as for people and places associated with the Greek Europa myth.
	Ganymede
	Ganymede is the largest moon in the solar system. Bigger than the planet Mercury, it’s the only moon with a known magnetic field. Like Europa, Ganymede is an ocean world, meaning that an alien sea sloshes beneath its icy crust. The moon’s ancient surface is covered in craters and peculiar grooved terrains, the source of which is not yet clear.

	Discovered in 1610 by Galileo Galilei, it was named for a boy who was made cupbearer for the ancient Greek gods by Zeus. Features are named after characters and places in the ancient mythologies of Egypt and the Fertile Crescent and for astronomers who discovered Jovian satellites.
	Callisto
	The third largest moon in the solar system, Callisto is about the size of the planet Mercury. Its icy surface is among the oldest and most cratered in the entire solar system, and scientists suspect that a global ocean lies beneath that battered terrain. Of the Galilean satellites, Callisto is the farthest from Jupiter and the planet’s punishing radiation, meaning that it could be an ideal place to build a base for future exploration of the Jovian system.

	Discovered in 1610 by Galileo Galilei, it was named after a lover of Zeus. Features are named for characters and places from Norse mythology.
	Saturn System
	Beautiful, ringed Saturn holds roughly five dozen natural satellites in its gravitational clutches—a moon menagerie that includes some of the most alien worlds in the solar system. From hazy Titan to eruptive Enceladus to a swarm of small ice balls like Tethys, the Saturnian satellites are among the most varied of any planetary retinue.

	7MAJOR MOONS54MINOR MOONS
	Mimas
	Tiny Mimas is the smallest and innermost of Saturn’s large moons, but it’s not quite big enough to be completely round. Instead, Mimas has a somewhat oval shape. A large 80-mile-wide crater, named after William Herschel, dominates the Mimantean surface. The crater’s relative size and pronounced central peak make Mimas resemble the Death Star from Star Wars.

	Discovered in 1789 by William Herschel, it was named after a titan in Greek mythology. Features are named for people and places in Sir Thomas Malory's Le Morte d'Arthur.
	Enceladus
	Enceladus may be small, but it is a mighty world. Geysers erupting from large fractures in the moon’s south pole spray salty water that ultimately forms one of Saturn’s rings. Those geysers are fueled by an ocean tucked beneath the moon’s crust. Because this small world contains all the ingredients needed for life as we know it, scientists consider Enceladus to be one of the prime places to search for extraterrestrial life in the solar system.

	Discovered in 1789 by William Herschel, it was named after a titan in Greek mythology. Features are named for people and places from the Arabian Nights collection of folktales.
	Tethys
	Tethys is a world made almost purely of ice, and it is the second brightest Saturnian ice ball after Enceladus. Its surface boasts two prominent features: large, shallow Odysseus Crater, measuring some 250 miles across; and a two-mile-deep valley called Ithaca Chasma that stretches more than 1,200 miles from end to end—or roughly 75 percent of Tethys’s circumference.

	Discovered in 1684 by Giovanni Domenico Cassini, it was named after a titan in Greek mythology. Features are named after people and places in Homer's Odyssey.
	Dione
	Dione is a heavily cratered world with an extremely tenuous oxygen atmosphere. Its trailing hemisphere is heavily fractured by what scientists once called “wispy terrain.” Close-up views from the Cassini spacecraft revealed those bright wisps to be chains of immense ice cliffs reaching hundreds of feet high.

	Discovered in 1684 by Giovanni Domenico Cassini, it was named after a titan in Greek mythology. Features are named after people and places from Virgil's Aeneid.
	Rhea
	Rhea is Saturn’s second largest satellite and one of the few moons suspected to have had a ring of its own, although the evidence for that is controversial. Its heavily cratered surface contains several features resembling the icy cliffs on Dione, and in 2010, the Cassini spacecraft revealed that Rhea has an extremely tenuous exosphere of oxygen and carbon dioxide.

	Discovered in 1672 by Giovanni Domenico Cassini, it was named after a titan in Greek mythology. Feature names are derived from various world creation myths.
	Titan
	The second largest moon in the solar system, Titan is one of the strangest natural satellites. Shrouded in a thick, hazy atmosphere, the moon is covered by rock-hard ice and is the only body other than Earth with permanent lakes, rivers, and seas dampening its surface. But on Titan, these liquids aren’t water—they’re oily, slick hydrocarbons. Scientists consider Titan to be one of the best places to look for truly alien life—organisms that evolved based on a completely different chemistry from our own.

	Discovered in 1655 by Dutch astronomer Christiaan Huygens, it was named after the race of powerful deities descended from Gaia and Uranus. The largest surface features refer to sacred or enchanted lands from various mythologies.
	Iapetus
	The farthest-flung of Saturn’s major moons, two-toned Iapetus is shaped like a walnut with slightly flattened poles. It also has a massive, still mysterious mountain ridge running straight as an arrow along 75 percent of its equator. The moon’s back side is a bright, gleaming white, while its front half is darker than coal; scientists recently determined that these colors are partially caused by dark dust, shed by another Saturnian moon, sticking to Iapetus’s face.

	Discovered in 1671 by Giovanni Domenico Cassini, it was named after a titan in Greek mythology. Features are named after people and places from Dorothy L. Sayers's translation of Chanson de Roland.
	Uranus System
	Ice giant Uranus is swarmed by more than two dozen moons, five of which are notably large. These major moons were known before Voyager 2 swung by the tipped-over planet in 1986 and discovered 10 additional satellites. Since then, though, no spacecraft has visited Uranus or Neptune, meaning that imagery from these planets’ neighborhoods is sparse, and astronomers have had to rely on Earth-based telescopes to study both systems.

	5MAJOR MOONS22MINOR MOONS
	Miranda
	Miranda is the smallest of the five major Uranian moons. Its surface is a conglomerate of broken terrains that look as though they’ve been haphazardly stitched together, including a gargantuan, 12-mile-high cliff and a landscape of chevron-shaped ridges. One potential explanation for Miranda’s weirdness is that at some point in its history, a giant impact nearly blew the moon apart, but gravity pieced it back together.

	Discovered by Gerard Kuiper in 1948, it was named after a character from Shakespeare’s The Tempest. Features are named after people and places from The Tempest.
	Ariel
	Ariel is one of the smallest of Uranus’s major moons—similar in size to Saturn’s moon Dione—but it is the brightest. Its surface is a patchwork of cratered terrain, canyons, ridges, and plains, and portions of it appear to be geologically young. The moon’s density suggests it is made of roughly equal parts rock and ice.

	Discovered in 1851 by William Lassell, it was named after a character in William Shakespeare’s The Tempest and Alexander Pope's The Rape of the Lock. Features  are named for light spirits.
	Umbriel
	Umbriel is the darkest of Uranus’s five major moons, although the source of its shadowy color is mysterious. Its surface is heavily cratered, suggesting that it isn’t geologically active. Umbriel’s darkness is disrupted by several curiously bright, vaguely bluish patches, which scientists suspect are frost deposits.

	Discovered in 1851 by William Lassell, it was named after a malevolent spirit in Pope’s The Rape of the Lock. Features are named for dark spirits.
	Titania
	Titania is the largest of the Uranian satellites and is the eighth most massive moon in the solar system. Like its other large siblings, Titania is probably made of equal parts rock and ice. Its surface bears fault valleys and other signs of early activity, suggesting that the moon was once a geologically active world.

	Discovered by William Herschel in 1787, it was named after a character in Shakespeare’s A Midsummer Night’s Dream. Feature names are derived from female characters in Shakespearean plays.
	Oberon
	Of Uranus’s five major moons, Oberon orbits the farthest from the planet. Its surface is the most cratered of all its siblings, with its largest gouge marks named Hamlet, Macbeth, and Romeo. The moon’s icy face is also riven by canyons, and it has a massive peak that stretches nearly seven miles high.

	Discovered by William Herschel in 1787, it was named after a character in Shakespeare’s A Midsummer Night’s Dream. Features are named after heroes and places in Shakespearean tragedies.
	Neptune System
	Of the Neptunian moons, Triton is perhaps the most magnificent. This bizarre, frigid object was probably born in the Kuiper belt, a vast swath of fractured, icy worlds beyond Neptune’s orbit. But at some point in Neptune’s history, the massive planet’s gravity captured the icy giant and claimed it as a moon.

	1MAJOR MOON13MINOR MOONS
	Triton
	Neptune’s largest moon, Triton, is an active world, with smooth volcanic plains and evidence for geysers erupting on its surface. It’s about twice as dense as water, which means Triton is largely rocky, and it’s swaddled in a thin atmosphere that’s mostly made of nitrogen and methane. Unlike every other large moon in the solar system, Triton orbits Neptune in retrograde, meaning that it moves in the opposite direction as the planet’s spin. That strongly suggests that Neptune stole Triton, perhaps yanking it from the nearby Kuiper belt.

	Discovered in 1846 by William Lassell, it was named after the son of Poseidon. Features are named after aquatic spirits and terrestrial aquatic features.
	Pluto System
	Among the solar system’s five official dwarf planets, Pluto has the most complex system of moons. Its largest companion, Charon, is so big it basically forms a binary system with Pluto. Its four smaller moons—Styx, Nix, Kerberos, and Hydra—are thought to be collisional shrapnel left over from the cataclysmic impact that made Charon. The dwarf planets Eris, Makemake, and Haumea also have known natural satellites, but these small moons are so dim and distant that they are essentially mysteries.

	1MAJOR MOON4MINOR MOONS
	Charon
	Charon is Pluto’s largest moon, and it’s so massive that the two bodies form a binary system with a shared center of mass between them—the only such pairing known in our solar system. In contrast to Pluto, Charon is dark and fractured, with massive canyons cracking its icy face. Its northern pole is distinctly red, dyed by organic compounds that have migrated over from Pluto’s atmosphere.

	Discovered in 1978 by James Christy and Robert Harrington, it was named after the underworld’s ferryman in Greek mythology. Features are named after people, places, and things from tales of exploration.
	In real life, Simba’s mom would be running the pride
	A lion expert breaks down lion family dynamics.
	A LION PRIDE is all females all the time. They catch the vast majority of the food, and they guard the territory from intruders—mostly other females that live nearby looking to expand their own territories.

	“Females are the core. The heart and soul of the pride. The males come and go,” says Craig Packer, one of the world’s leading lion researchers and director of the Lion Research Center at the University of Minnesota.

	But unless you’re a Lion King superfan, it’s a pretty good bet that you can’t remember the name of Simba’s mom. All the male lions have central and memorable roles in the film: Simba, the main character destined to become king. Mufasa, his father, who dies when Simba’s uncle, the black-maned Scar, plots to take his throne. But Simba’s mother? What was her name again?
	Her role in the film is so inconsequential she’s easy to forget. Her name is Sarabi. And if she was the queen of a real-life lion pride, she’d take on so much more than a supporting role. In fact, lion prides are matrilineal societies where the males barely stick around long enough to form the types of familial relationships shown in the Disney film, an all-new version of which comes out this July. (The Walt Disney Company is majority owner of National Geographic Partners.)

	“Females define their territory. They’ve grown up there and have been listening to neighbors roaring their whole lives,” says Packer, a National Geographic Society grantee. And if their pride gets too big, the females will even carve out a new territory next door for their daughters to take over and start their own pride. Ninety-nine percent of all the members of a lion pride are related females, he says.

	Males come and go
	The males, on the other hand, are transitory. They come and go, mostly spending their time fighting each other and teaching male cubs how to survive when they eventually leave the pride. Males can’t stay in the pride they’re born into because they’re related to all of the lions there.

	“Let’s say Simba comes back, and his grand prize for coming home and being the hero of the pride is he gets to marry Nala. But guess what—she’s his sister. Ewwww,” says Packer. “If he did come home and became the resident male, he wouldn’t just be having it off with his sister. It would also be his aunts, his mother, grandmother, cousins. All the females in the pride.”

	That’s why, for the sake of genetic diversity and as a way to avoid life being generally very gross, male lions always leave and find a new pride. “Simba would have left and never come back,” he says. (Read about the world's largest lion relocation.)

	Male lions rarely travel alone. Because competition for prides is so fierce, all male lions travel with one or more other males so they can protect each other. “You have to have a partner in arms to withstand the challenges of all the other males that want to take over your family and kill your babies,” says Packer. He notes that the competition between Mufasa and Scar wouldn’t make sense in the real world because, without each other to depend on, their pride would just be taken over by another coalition of males.

	In fact, Packer says, the competition for a pride is so fierce that a team of male lions is rarely able to stick around for more than two or three years (maybe a bit longer, he says, if there are four or five them). “Males’ life is fast and furious,” he says. It’s a circular existence—they make babies, “they occasionally rouse themselves and catch really big prey like a buffalo or giraffe, though it doesn't require any strategy—they just knock it over,” he says, and they fight off other males for as long as they can before they lose that battle and move on to another pride.

	Quality manes
	Meanwhile, the sisterhood of the pride continues more or less unhindered by which males happen to be around at any moment. And, in fact, the females have agency in deciding which males they’re interested in having around. “If it wasn’t for the females, there would be no reason for the males to have manes. Females prefer the male who is the most conspicuous and has the clear characteristics they can rely on to ensure their babies are going to survive and be healthy,” he says.

	Packer also points out that, though the childless villain Scar had a black mane in the film, in the real world it would be Mufasa with a black mane, because that’s what the ladies like.
	“Growing a black mane is a signal you’re genetically superior,” he says. Black manes are hotter and heavier than a traditional mane and, because lions are so muscular, they are extremely prone to overheating. A black mane indicates “good physical condition, higher levels of testosterone, and they’re more likely to withstand being wounded,” he says, because it means they have a genetic ability to fight off parasites. (See a rare black-maned lion caught on film—and a rare white one, too.)

	“There’s no gene for the dark mane,” says Packer. “The color of the mane can vary through time depending on how healthy they are, and if things get really bad their manes can fall out completely. The mane is a signal of quality.”

	In the end, just about every part of the pride’s existence, all the way down to how the males look, is in service of the Lion Queens.
	How are tree rings used to help date an archaeological site?
	Dendrochronology is an invaluable tool to help scientists determine the age of ancient settlements and artifacts.
	Archaeologists have a group of unlikely allies: trees. Dendochronology, the scientific method of studying tree rings, can pinpoint the age of archaeological sites using information stored inside old wood. Originally developed for climate science, the method is now an invaluable tool for archaeologists, who can track up to 13,000 years of history using tree ring chronologies for over 4,000 sites on six continents.

	Trees don’t grow their trunk uniformly; though they add a new ring each growing season, trunk growth is closely linked to climate conditions. Under ideal conditions, trees grow quickly, leaving wide annual rings behind. During droughts, unseasonable cold, and other unusual conditions, growth slows, leaving behind narrow rings.
	In the early 20th century, astronomer Andrew Ellicott Douglass began studying trees in the American Southwest to learn more about how sunspots affected climate on Earth. When he realized that the rings of trees in the same area all had the same patterns, he decided to use them as a record of the area’s historical climate.

	Douglass eventually extended his work from living trees to wood used in ancient pueblo sites and began using them to piece together a regional chronology that could be used to date such archaeological sites. His research, which was partially funded by the National Geographic Society, helped push back the previously suspected dates for pueblos and changed the way archaeologists saw excavation sites. (Learn the other techniques archaeologists use to date sites and artifacts.)

	Today, dendochronology is a critical tool for helping date archaeological sites and artifacts. The term was derived from the ancient Greek words for tree (dendron) and time (khronos).

	When archaeologists recover timbers during excavations, they either cut full cross-section or retrieve cross-section cores, then compare them to regional chronologies to find matching ring patterns and determine a site’s age. Differing ages in specimens can reveal waves of construction at a particular site, or reveal migration and trade patterns with pieces of wood that were not cut locally.
	Dendochronology is more useful in some areas than others. In the tropics, for example, trees do not show distinct seasonal patterns, which makes tropical dendrochronology challenging. Wood must be well preserved to study effectively. And ancient people didn’t necessarily build with wood, depriving archaeologists of a critical tool for studying them.

	Nonetheless, the tool is used across disciplines like climatology and art history, and tree ring chronologies are even used to calibrate radiocarbon dating measurements. Laboratories like the University of Arizona’s Laboratory of Tree-Ring Research train researchers and conduct ongoing research.

	As tree ring data piles up, researchers have realized how valuable it can be. Tree ring patterns have recently been proven to match up with historical drought records and have revealed everything from changing indigenous forest management in the Central Amazon to the climate patterns that caused ancient Rome to rise and fall. Because of its cross-disciplinary use, dendrochronology has strengthened links between disciplines—and proven how valuable trees can be to archaeologists in search of more information about the sites they study.
	Emirati astronauts count down to ISS scientific mission
	After months of grueling training, the first Emirati astronaut will soon blast-off for a historic mission aboard the International Space Station (ISS).
	ON SEPTEMBER 25, 2019, Hazzaa AlMansoori, former Emirati F-16 aircraft pilot, will become the first UAE national in space aboard a Russian Soyuz MS-15 spacecraft.

	His journey will mark an important milestone in the UAE’s burgeoning space industry, led by the Mohammed Bin Rashid Space Centre (MBRSC).

	Fellow Emirati Sultan AlNeyadi is second in line to make the trip to the International Space Station (ISS).

	Last September, the pair were announced as the UAE’s first astronauts by Vice President and Prime Minister of the UAE and Ruler of Dubai, His Highness Sheikh Mohammed Bin Rashid Al Maktoum.

	Getting to this point has been a journey in itself. According to Assistant Director General for Science and the Technology Sector, and Head of the UAE Astronaut Programme, Salem AlMarri, a battery of tests were used to whittle down a field of thousands of eager applicants.

	“We looked at their capabilities in physics, math, science, technology and their knowledge about space”, says AlMarri.

	“That took us from 4,022 applicants down to about 400.

	“Then we had another process that took us down to the top 95, who went through a grueling medical process. From those we got the top 39, who completed the medical process and who were ready for the one-on-one interviews.”

	The 39 candidates then went through another round of tests covering intelligence, aptitude, cognitive ability, personality, and memory.

	A total of 18 passed that hurdle, securing themselves a spot in the final interview stage. The remaining elite of 9 hopefuls were then chosen to undergo an assessment by Russian space experts from Roscosmos.
	In April of 2019, it was announced AlMansoori, a father of four and graduate from Khalifa bin Zayed Aviation College, will train with the primary team to be blasted to the ISS. The hopes of a young and vibrant nation will go with him.

	AlMansoori’s dream of space started at a young age. “I used to stand out in the dunes at night and look up at the stars and wondered how to get there.

	“I remember in grade four I saw a book about an Arab astronaut, Prince Sultan bin Salman Al Saud, and I thought that’s it – it’s possible for me.

	“That’s why I applied to join the air force – it felt a little bit closer to the stars.”

	Preparing for the journey
	AlNeyadi, an information technology PhD will be part of the backup crew. Both astronauts continue to participate in a rigorous training regime at the Yuri Gagarin Cosmonaut Training Centre (GCTC) in Russia.

	The program includes parabolic flight training where both astronauts practice donning the 10kg Sokol space suit for 25 to 30 seconds in a microgravity environment, as well as sessions in a huge centrifuge to ready them for the high g-forces of launch and re-entry.

	Winter survival training, meanwhile, taught the astronauts how to stay alive in the brutal Russian winter in case they land in an unexpected location. They have also learned how to get out of a damaged capsule and build two types of shelters utilizing available resources.
	The pair have also applied themselves to learning the incredibly complex systems inside the Soyuz capsule, including the hyper-critical Launch Escape System (LES) which the crew would use to separate the capsule from the rocket in the event of an emergency.

	In October 2018, Russia was forced to abort a Soyuz rocket mission shortly after launch due to a technical fault. Fortunately, the crew members on board returned to Earth safely.
	It was one of the few incidents the stalwart Russian launch vehicle – the workhorse of Russia’s space program - has suffered.

	Despite the setback, AlNeyadi is not deterred by the risks involved in space travel.

	“The LES is an incredibly reliable and complex system… even just before launch you have the ability to escape.

	“Space is a tricky business and accidents can happen, but you have to trust in your knowledge and the equipment,” AlNeyadi says.

	Important work in orbit
	During his eight-day stay aboard the ISS, AlMansoori will study the impact of microgravity in 15 experiments that have been selected based on MBRSC’s ‘Science in Space’ competition, which involves schools in the UAE in the process of preparing experiments for space. Students will also conduct the same experiments on earth to compare the results with those done in space.

	Moreover, the reaction of the human body to space will be studied before and after the trip, the first time this kind of research will be done on an astronaut from the region. This is on top of the existing scientific missions assigned to AlMansoori whilst on the ISS’s, using on-board laboratories.

	AlMansoori says he hopes to inspire the nation by hoisting the UAE flag inside the International Space Station when he comes aboard.

	“It will be a great honor to be the first Emirati astronaut to reach the ISS,” he says.

	“I applied for the program because I had a dream as a boy and our leaders encourage us to achieve our dreams.”
	Discover more about the UAE’s Mohammed Bin Rashid Space Centre and their planned space missions on our Reach for the Stars content hub.
	Innovations in insulin
	Building upon earlier discoveries continues to produce ever-better diabetes treatments.
	Diabetes is a disorder of carbohydrate metabolism whose earliest known documentation appears in an Egyptian text circa 552 BC. From then until the 20th century, a diabetes diagnosis was a death sentence realized within weeks or months. Without understanding the underlying mechanisms of the disease, physicians could only treat symptoms. Often, the prescription for diabetes was a “starvation diet,” which eliminated all but the most necessary carbohydrates.
	Fast forward to today, and diabetes is a chronic condition with which patients can live full lives. The evolution of diabetes from fatal disease to manageable condition demonstrates how the process of science is iterative, that is, continually circling back to build upon earlier discoveries. The iterative nature of science enabled diabetes researchers to dig deeper, expand their understanding, and produce ever-better treatments, including the genetically engineered insulin therapies used today.

	In 1890, researchers discovered the link between the pancreas and diabetes symptoms. Normally, the pancreas makes an important protein called insulin, a hormone that regulates blood glucose, or sugar, the body’s main source of energy. In people with diabetes, the body does not make enough insulin, produces none at all, or does not properly use insulin. This causes high blood glucose levels—a dangerous condition that can lead to heart disease, painful diabetic neuropathy (a gradual deterioration of peripheral nerves), blindness, kidney disease or failure, and lower-extremity amputations.
	In the early 1920s, Frederick Banting and his student assistant Charles Best discovered insulin under the directorship of John Macleod at the University of Toronto. With further help from James Collip, the research team developed a purified extract from animal pancreatic insulin cells that could be injected into people with diabetes. Banting and Macleod were awarded the Nobel Prize in Physiology/Medicine in 1923. The new medicine markedly extended the lives of people with diabetes.

	While diabetes was no longer a death sentence, the early insulin medications were extracted from animals and, therefore, difficult to produce, painful to inject, and prone to triggering allergic reactions. As the field of medicine advanced, improvements in the duration of action, effectiveness, and safety of insulin proved to be a bellwether for overall modern medical progress.

	Insulin played a prominent role in several scientific discoveries. For example, insulin was the first protein to be sequenced, a process by which scientists examine its amino acid structure. The achievement, accomplished in 1955 by British biochemist Frederick Sanger—and garnering him the Nobel Prize in Chemistry in 1958—launchedan area of science that has been hugely important to the discovery of new medicines.
	In 1963, insulin became the first protein to be chemically synthesized. Fifteen years later, when researchers discovered how to genetically alter bacteria to produce an exact copy of human insulin, it became the first human protein to be manufactured through biotechnology. The achievement meant scientists no longer had to rely on animals to produce the medicine, reducing allergic reactions and improving effectiveness. It also meant insulin was part of the earliest wave of biologics, a growing field of medicine that is changing the way we treat many autoimmune conditions such as asthma and juvenile arthritis, and diseases including cancer.

	In recent decades, researchers have refined and honed insulin to make it more stable, longer-lasting, and easier to inject. In the 1996, the Federal Drug Administration approved the first precisely engineered insulin analog, which is more physiologic and closely mimics the behavior of insulin produced in a patient without diabetes. Engineered insulin analogs improved diabetes control and reduced or delayed complications. Today, biopharmaceutical research companies continue to research new options for diabetes patients.

	Insulin provides an excellent example of the iterative nature of medical research. Its discovery, for example, laid the foundation for protein sequencing, which provided the map necessary for the complex process of protein synthesis. In addition, today’s insulin medications and diabetes research incorporate advancements in genetics—a field believed to be in its earliest stages of changing how we design and produce new medicines.

	Learn more about biopharmaceutical innovation, including the breakthrough discoveries used to treat diabetes.
	The science behind California’s two big earthquakes
	The pair of powerful temblors that shook the United States’ West Coast promise fresh clues about the region’s complex geology.
	On the morning of July 4, a magnitude 6.4 rocked Southern California, fracturing roads and sending people fleeing to safety. But that wasn’t all the Earth had in store: Less than a day and a half later, a powerful magnitude 7.1 temblor shook the region.

	While earthquakes are not unexpected, the two most recent temblors are the largest that have struck this area in decades. And they promise to yield fresh clues about its complex geology.

	The duo of quakes struck in what’s known as the Eastern California shear zone—an area east of the infamous San Andreas fault, where the Pacific Plate grinds against the North American Plate, creeping northwest at roughly two inches each year. The area extends from the southern Mojave Desert, up the eastern side of the Sierra Nevada, and into western Nevada. It’s crisscrossed by fractures in the Earth caused by the movement along the nearby tectonic plate boundary.

	“The Eastern California shear zone is a really interesting area,” says Wendy Bohon, an earthquake geologist at the Incorporated Research Institutions for Seismology (IRIS). “How is it working? How is it accommodating plate motion? What are going to be the big structures that come out of this millions of years down the road?”
	What happened?
	The recent events are what’s known as strike-slip earthquakes, which occur when two blocks of Earth shift side-by-side, grinding past each other. They seemed to have occurred along the same set of faults, located in an area known as the Little Lake fault zone.

	While no deaths or major injuries have yet been reported, the intensity of the ground movement was quite strong—enough to send goods flying off store shelves and buildings swaying. The shaking was also widespread, with reports of light ground movements as far as Chico, California, and Phoenix, Arizona.

	Of particular interest in these quakes is that at least the first temblor seemed to have simultaneously broken two sections of faults that cut across each other at nearly a right angle. While such complex quakes are not unheard of, recent research suggests that they may be more common than once believed, explains Zachary Ross, a geophysicist at the California Institute of Technology.

	“Historically, the thought has been that earthquakes occurred on individual faults,” he says. “And then over time, as the data has gotten better and better, we’ve started to realize that there’s potential for multiple faults to rupture for single events.”
	This shift in thinking was propelled by the magnitude 7.3 earthquake that shook Landers, California, in 1992. This temblor fractured along at least five fault segments. Subsequent earthquakes have revealed similar complex breakage, including the magnitude 7.2 earthquake in 2010 in Baja California, Ross notes. This latest quake is further evidence that this complexity is common, even for smaller magnitude events.

	Why did two big earthquakes strike?
	In most circumstances, big earthquakes strike in a familiar sequence: There’s a large earthquake followed by a series of smaller events. That’s because the movement that occurs during a large earthquake causes increased strain in the surrounding region. Earthquakes are the Earth’s way of relieving this strain.

	But in some circumstances, such as the recent pair of earthquakes in California, a relatively large temblor might just be the forerunner for even bigger event. While the difference between 7.1 and 6.4 may seem minor, magnitude is a logarithmic scale. An increase of a unit of magnitude is about 32 times more energy, which means that the second earthquake released roughly 11 times the energy of the first temblor.

	Scientists think of this series of earthquakes as the foreshock, the mainshock or strongest event, and then the aftershocks, explains Susan Hough, a seismologist with the U.S. Geological Survey.

	“But it’s way too simplistic,” she notes.

	Every earthquake causes a shift in the landscape, redistributing the strain in the crust, which means all earthquakes could trigger other earthquakes. “Whether or not an earthquake itself is an aftershock,” she says, “you can think of it as a potential [earthquake] parent.”

	The likelihood that a big earthquake will trigger a larger event is roughly one in 20, according to Hough. That’s definitely a low risk. “But that one in 20 isn’t zero,” she says.

	Scientists, however, are still untangling the connection between the recent events. The magnitude 7.1 temblor seemed to break along the the northwest-southeast limb of the pair of faults that ruptured earlier, Hough notes, extending farther than the first quake in both directions.

	“Major scientific questions: Did part of the fault break and then break again? How did breaks compare in detail?” she writes on Twitter. Finding the answers to these questions could have important implications for understanding the hazards of these events.

	“This appears to be the clearest case I've seen that that did indeed happen,” she says of the same fault rupturing more than once in a short time period. “But much work is needed to sort out the details.”

	What’s going to happen next?
	So far, many more than a thousand aftershocks have rippled through the region. While the frequency and intensity of subsequent temblors will wane, Southern California likely has more shaking in store. “A magnitude 7 is going to produce activity for years,” Ross says.

	The USGS estimates that over the course of the next week, between 240 and 410 earthquakes of magnitude 3 or higher will likely ripple through the region. These are events just large enough to feel if you are positioned close to their epicenter. As for larger earthquakes, the probability becomes increasingly small, but not out of the question.

	The USGS estimates that there’s a 24 percent chance that as many as two earthquakes magnitude 6 or larger could strike in the next week. But as scientists continue to analyze these events, they may adjust those numbers.

	Such a strong series of aftershocks is not unexpected. “Earthquakes out in the Mojave Desert traditionally have these bigger, robust aftershocks sequence,” Bohon says.

	She cautions, however, that all of these probabilities are forecasts and not predictions. No one, despite what some claim, can predict future earthquakes. She likens the difference to weather forecasts, which are an estimate of the probability that something might happen.

	“You would never predict the weather,” she says. “You can’t predict with 100 percent certainty that it [will be] raining unless it’s raining. And earthquakes are very similar.“

	Bohon emphasizes that it’s normal to be scared during an earthquake, which can rock the ground like a boat in rough water. But as aftershocks ricochet through Southern California, she suggests people who live in earthquake-prone regions check how prepared they are for the next event.

	“This was a good scenario earthquake,” says Bohon. “It was in a fairly unpopulated area, but a lot of people felt it.” Bohon hopes that means those who experienced it are getting ready for the next one.

	“It’s okay to be scared,” she says, “but we also have to be prepared.”
</documents>